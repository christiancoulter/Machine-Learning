{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1d7R1v7Ie0EYkwSBwv8_hG4ad1fWmXmil","timestamp":1703129190951},{"file_id":"1HKzoP0RV5sS4bq29P8wSL8TiesrF4y3J","timestamp":1582783503697},{"file_id":"1XaQBpkbanCXFxxJ1ghT9e4CqFstOCF8G","timestamp":1582585052121},{"file_id":"1bUVTGtdPQASsA0heGjrU-N3WmBWuB6u_","timestamp":1582497019436},{"file_id":"1rGXFqRqm039V8CFbgHhJIelkvjMtjccM","timestamp":1581473139504},{"file_id":"1LvZxCmUnjHLiPXq5jdvyQionnTMMYItf","timestamp":1581323022958}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8iwaee_T-Urj"},"source":["# ASSIGNMENT 4 - INTRO TO MACHINE LEARNING | Resampling, Model Evaluation, Feature Selection and Regularization\n","\n","\n","> **FULL MARKS = 160**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GhorwbNcOblB"},"source":["\n","\n","1.2. Leave-One-Out Cross Validation(LOOCV) | SCORE :\n"]},{"cell_type":"markdown","metadata":{"id":"IVKgB-iQ-nYY"},"source":["**Note:** To submit the assignment, please follow the same steps and in assignments 1, 2, & 3.\n","\n","In this assignment we will use things we have learned from previous exercises. You will not be given instructions on how to load data, plot data, standardize/normalize data, how to write functions and many more. You will be given instructions on what you will be doing. **SO PLEASE START THIS ASSIGNMENT AS EARLY AS POSSIBLE**\n","\n","1. **Resampling Methods | SCORE : 40**\n","  \n","  **1.1 K-Fold Cross Validation**\n","      \n","    References\n","    > Please follow lecture notes\n","\n","  **1.2 Leave-One-Out Cross Validation(LOOCV) | SCORE :**\n","      \n","    References\n","    > Please follow lecture notes\n","\n","\n","2. **Model Evaluation | SCORE : 35**\n","  \n","  **2.1 Confusion Matrix:**\n","      \n","    References\n","    > Please follow references on previous assignments\n","\n","  **2.2 Metrics : Accuracy, Precision, Recall, F1-Score and ROC-Curve**\n","      \n","    References\n","    > https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n","    > https://towardsdatascience.com/receiver-operating-characteristic-curves-demystified-in-python-bd531a4364d0\n","    > https://www.daniweb.com/programming/computer-science/tutorials/520084/understanding-roc-curves-from-scratch\n","\n","3. **Linear Model Selection | SCORE : 40**\n","  \n","  **3.1 Subset Feature Selection**\n","      \n","    References\n","    > https://archive.ics.uci.edu/ml/datasets/Online+Video+Characteristics+and+Transcoding+Time+Dataset#\n","\n","  **3.2 Forward stepwise feature selection**\n","      \n","    References\n","    > Please follow lecture ntoes\n","\n","4. **Model Regularization | SCORE : 45**\n","  \n","  **4.1 Ridge Regression - L2 Rgularization**\n","      \n","    References\n","    > https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html\n","\n","  **4.2 Lasso Regression - L1 Regularization**\n","      \n","    References\n","    > https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html\n"]},{"cell_type":"markdown","metadata":{"id":"NsUM8-i_KhK1"},"source":["### 1. Resampling Methods\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"id":"9ZQvDnEb38mi"},"source":["# Required Library are loaded for you\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn import preprocessing\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import matplotlib.ticker as ticker\n","from matplotlib import rcParams\n","# figure size in inches\n","rcParams['figure.figsize'] = 16,8"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DxRoCGJn375k"},"source":["# The following base class ResamplingMethods is implemented\n","# You will implemenet KFold and LOOCV class by inheriting this base class\n","# To make your work simpler we will show you a simple example\n","# by inheriting this class to make validation set resampling method"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8kV06jmj37SF"},"source":["class ResamplingMethods(object):\n","  def __init__(self, sklearn_databunch = load_iris(), test_size = 0.2, val_size = 0.2):\n","    \"\"\"\n","    :param dict sklearn_databunch: a databunch or dictionary that contains keys(data, target, feature_names, target_names, DESCR)\n","    :param float test_size: This represent percentage of data size to be selected as test set from given data\n","    :param float val_size: This represent percentage of data size to be selected as validation set from given data\n","    :return None\n","    \"\"\"\n","    # Data Initialization\n","    self.__x = preprocessing.scale(sklearn_databunch.get('data'))\n","    self.__y = sklearn_databunch.get('target')\n","    self.xlabel = sklearn_databunch.get('feature_names')\n","    self.ylabel = sklearn_databunch.get('target_names')\n","    self.description = sklearn_databunch.get('DESCR')\n","\n","    self.test_size = test_size\n","    self.val_size = val_size\n","\n","    # Do train-test split here\n","    # Please note one important remark here\n","    # self.xtest and self.ytest will be untouched and will be used only in final step of model validation\n","    # To implement resampling methods we will further split self.xtrain and self.ytrain into train and val set\n","    # So we can have different train-val set throughout different resampling implementation\n","    # Howevere, our test set will be keep fixed, this is our FINAL CHECK,\n","    # If our model doesn't perform well on FINAL_CHECK,\n","    # i.e, for perfect case(note that accuracy is acceptable), test_accuracy>=val_accuracy>train_accuracy\n","    self.xtrain, self.xtest, self.ytrain, self.ytest = train_test_split(self.__x,\n","                                                                        self.__y,\n","                                                                        test_size=self.test_size,\n","                                                                        random_state=4347)\n","\n","    self.record = {'data':[],'accuracy':[],'loss':[],}\n","\n","  # Create_model creates a logistic regression model with random state of our course_id\n","  def create_model(self,**kwargs):\n","    # Classifier Initialization\n","    self.model = LogisticRegression(random_state=4347)\n","\n","  # If you print an instance of this class it will print description of the dataset\n","  def __repr__(self):\n","    return self.description\n","\n","  # Run corresponding resampling methods implemented in inherited classes\n","  def run(self,**kwargs):\n","    return getattr(self,self.method)()\n","\n","  # Calculate and return logistic between y and y_(predicted values of y given x)\n","  def logistic_loss(self,x,y):\n","    \"\"\"\n","    :param ndarray x: input to model\n","    :param ndarray y: true label\n","    :return float: logistic_loss\n","    \"\"\"\n","    y_hat = self.model.predict_proba(x)\n","    y = np.eye(len(self.ylabel))[y]\n","    return -np.mean(y*np.log(y_hat)+(1-y)*np.log(1-y_hat))\n","\n","  def predict(self,*X):\n","    \"\"\"\n","    :param tuple of ndarray X: X contains number of different x's eg.X may be xtrain only or (xtrain,xval,xtest)\n","    :return list of ndarray: if X contains more than one x(i.e, xtrain,xval,xtest) returns list of [ytrain_predicted, yval_predicted,ytest_predicted]\n","    \"\"\"\n","    return [*map(lambda x:self.model.predict(x),X)]\n","\n","  def accuracy(self,*XY):\n","    \"\"\"\n","    :params pair tuple of ndarray XY: eg. ((xtrain,ytrain),(xval,yval),(xtest,ytest))\n","    :return list of accuracy: eg.[train_accuracy,val_accuracy,test_accuracy]\n","    \"\"\"\n","    return [*map(lambda xy:self.model.score(*xy),XY)]\n","\n","  def loss(self,*XY):\n","    \"\"\"\n","    :params pair tuple of ndarray XY: eg. ((xtrain,ytrain),(xval,yval),(xtest,ytest))\n","    :return list of loss: eg.[train_loss,val_loss,test_loss]\n","    \"\"\"\n","    return [*map(lambda xy:self.logistic_loss(*xy),XY)]\n","\n","  def fitmodel(self,xtrain, ytrain, **kwargs):\n","    \"\"\"\n","    :param ndarray xtrain: input to model\n","    :param ndarray ytrain: true label\n","    \"\"\"\n","    self.create_model()\n","    self.model.fit(xtrain, ytrain)\n","\n","  def get_report(self):\n","    \"\"\"\n","    calculate average loss and accuracy\n","    \"\"\"\n","    report = self.record.groupby(['data']).mean()[['accuracy','loss']]\n","    report['method'] = self.method\n","    return report\n","\n","  def visualize(self, **kwargs):\n","    \"\"\"\n","    :param dict kwargs:contain argument for plotting\n","    \"\"\"\n","    config = dict(data=self.record,\n","                  x=self.method,\n","                  y=\"accuracy\",\n","                  hue=\"data\",\n","                  size=\"loss\",\n","                  sizes=(40, 400),\n","                  alpha=.5,\n","                  aspect=3)\n","    config.update(**kwargs)\n","    sns.relplot(**config)\n","    plt.title(f'Visualization for CV Method : {self.method}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9m0GeneRHAzr"},"source":["# The following is a simple implementation of Validation Set Approach\n","# Please follow the lecture notes in given references to understand the validation set approach\n","# We will now create a class ValidationSet by inheriting Base Class ResamplingMethods\n","# We need to implement two important functions here\n","# Function train_val_split will return xtrain,xval,ytrain,yval\n","# Function valset will perfom 3 important tasks\n","                  # 1. Create a record instance, which will keep track of accuracy and loss and finally convert this instance to DataFrame\n","                  # 2. Fit N random steps (in each step we do random train_test split which you can see in function train_val_split, without specifying random_state)\n","                  # 3. Calculate Accuracy\n","# All other methods will be reused from base class(you can think of this as an extension class to base)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k_1odqGIRDBc"},"source":["class ValidationSet(ResamplingMethods):\n","  def __init__(self,fit_steps = 10):\n","    \"\"\"\n","    :params int fit_steps: This is a number of times model needs to fit with random split of data(except test) to xtrain and val set\n","    \"\"\"\n","    super(ValidationSet, self).__init__()\n","    self.fit_steps = fit_steps\n","    self.method = 'valset'\n","\n","  def train_val_split(self, **kwargs):\n","    \"\"\"\n","    :params ndarray x: x is a feature that should be splitted into further train-val sets\n","    :params ndarray y: x is a label that should be splitted into further train-val sets\n","    :return tuples of ndarray\n","    \"\"\"\n","    x = kwargs.get('x', self.xtrain)\n","    y = kwargs.get('y', self.ytrain)\n","    xtrain, xval, ytrain, yval = train_test_split(x,y,test_size = self.val_size, random_state = kwargs.get('random_state'))\n","    return xtrain, xval, ytrain, yval\n","\n","  def valset(self,**kwargs):\n","    self.record.update({self.method:[]})\n","    # Run fit_steps time\n","    # Each time perform split on self.xtrain and self.ytrain to produce xtrain,xval,ytrain,yval\n","    # Each time fit new model with xtrain,ytrain]\n","    # Calculate accuracy on (xtrain,ytrain),(xval,yval),(self.xtest,self.ytest)\n","    # Calculate loss on (xtrain,ytrain),(xval,yval),(self.xtest,self.ytest)\n","    # Record all metrics\n","    # After done convert record into dataframe, which will be used for visualization\n","    for i in range(self.fit_steps):\n","      xtrain, xval, ytrain, yval = self.train_val_split(**kwargs)\n","      self.fitmodel(xtrain,ytrain)\n","      self.record['data'].extend(['train','test','val'])\n","      self.record['accuracy'].extend(self.accuracy((xtrain, ytrain),(xval,yval),(self.xtest,self.ytest)))\n","      self.record['loss'].extend(self.loss((xtrain, ytrain),(xval,yval),(self.xtest,self.ytest)))\n","      self.record[self.method].extend([i]*3)\n","\n","    self.record = pd.DataFrame(self.record)\n","\n","# Now here Lets Create an instance of validation set with fit steps = 10\n","vs = ValidationSet(fit_steps = 10)\n","# Lets see the data description\n","print(vs)\n","# Now run this model\n","vs.run()\n","# Now Let us Visualize this model\n","vs.visualize()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vyOkwgrKML85"},"source":["***EXERCISE NO. 1***\n","\n","  > **Task-1 | Score : 10**"]},{"cell_type":"code","metadata":{"id":"uQJXxY1vL_Ks"},"source":["# Now you will implement KFold Methods\n","# Please follow the lecture notes in the given references to understand how KFold set approach\n","# You will now create a class KFold by inheriting Base Class ResamplingMethods\n","# You need to implement two important function here, First two function are implemented for you\n","                  # 1. create_k_slices : This function will take k,N(optional, default : length of self.xtrain),window(optional, default : length of self.xtrain // k)\n","                                         # and return list of slices, eg. for k = 10, and length of self.train=120, [slice(0,12),slice(12,24),.....,slice(108,120)]\n","                  # 2. get_kth_mask : This function will take k, which represents train_val set of kth fold and return train_mask and val_mask\n","                  # 3. train_val_split : This function will take k, which represents train_val set of kth fold, calculate masks and return xtrain,xval,ytrain,yval\n","                  # 4. kfold : this function will run a loop k steps, each steps fit model on kth fold and record results and finally replace record as dataframe\n","# Now fill empty code section wherever asked"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HSGlE5ehTSgC"},"source":["class KFold(ResamplingMethods):\n","  def __init__(self,k = 10):\n","    super(KFold, self).__init__()\n","    \"\"\"\n","    :params int k: This is a number of times model needs to fit with k fold dataset\n","    \"\"\"\n","    self.k = k\n","    self.create_k_slices(k)\n","    self.method = 'kfold'\n","\n","  def create_k_slices(self, k =10, **kwargs):\n","    N = kwargs.get('N',len(self.xtrain))\n","    window = kwargs.get('window', len(self.xtrain)//k)\n","    self.slices = [slice(*i) for i in zip(range(0,N+1,window),range(window,N+1,window))]\n","\n","  def get_kth_mask(self, k=0):\n","    assert k<len(self.slices), '!!!K CANNOT BE MORE THAN NUMBER OF SLICES!!!'\n","    indices = np.arange(0,len(self.xtrain))\n","    val_mask = (self.slices[k].start <= indices)*(self.slices[k].stop > indices)\n","    train_mask = np.logical_not(val_mask)\n","    return train_mask,val_mask\n","\n","  def train_val_split(self,k):\n","    # calculate train and val mask\n","    train_mask, val_mask = ??\n","    # Now you will be selecting xtrain and xval set from self.xtrain\n","    # Use indexing mask and select required data\n","    # Note: xtrain, xval are splitted from self.xtrain using mask index so xtrain is not self.xtrain but a slice of it\n","    # select xtrain using train mask\n","    xtrain = ??\n","    # select xval using val mask\n","    xval = ??\n","    # select ytrain using train mask\n","    ytrain = ??\n","    # select yval using val mask\n","    yval = ??\n","    # return xtrain,xval, ytrain, yval\n","    return xtrain,xval,ytrain,yval\n","\n","  def kfold(self,**kwargs):\n","    self.record.update({self.method:[]})\n","    for i in range(self.k):\n","      # calculate xtrain,xval,ytrain,yval for ith fold\n","      xtrain, xval, ytrain, yval = ??\n","      # fit model with xtrain and ytrain\n","      ??\n","      self.record['data'].extend(['train','test','val'])\n","      # record results for train-val-test sets\n","      self.record['accuracy'].extend(??)\n","      self.record['loss'].extend(??)\n","\n","      self.record[self.method].extend([i]*3)\n","    # Convert results into dataframe\n","    self.record = pd.DataFrame(self.record)\n","\n","\n","\n","# Now here Lets Create an instance of KFOld set with k = 10\n","kf = KFold(k = 10)\n","# Now run this model\n","kf.run()\n","# Now Let us Visualize this model\n","kf.visualize()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MclWG9xUQ5G7"},"source":["***EXERCISE NO. 1***\n","\n","  > **Task-2 | Score :5**"]},{"cell_type":"code","metadata":{"id":"YgNHdRkLQ5HB"},"source":["# Now you will implement LOOCV(Leave One Out Cross Validation) Method\n","# Please follow lecture notes in given references to understand how LOOCV set approach\n","\n","# LOOCV is a special case of KFold with k is as equal to train size and validation set will have size = 1(that is why leave one out)\n","# So we will now Inherit KFold Class instead of Base\n","# You have to fit your model k times(that is as much as train size is)\n","# You have to make one small change in __init__ to make"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BVKhgld9mFa4"},"source":["class LOOCV(KFold):\n","  def __init__(self):\n","    super(LOOCV, self).__init__()\n","    self.method = 'leaveoneout'\n","    # Make that small change here\n","    self.k = ??\n","    ??#create_k_slices_using_k_ you can use function defined in BaseClass(KFold)\n","\n","\n","  def leaveoneout(self):\n","    self.record.update({self.method:[]})\n","    # just call default kfold method\n","    return self.kfold()\n","\n","# Now here Lets Create an instance of LOOCV\n","lo = LOOCV()\n","# Now run this model\n","lo.run()\n","# Now Let us Visualize this model\n","lo.visualize()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TsgncCfbbllf"},"source":["***EXERCISE NO. 1***\n","\n","  > **Task-3 | Score :10**"]},{"cell_type":"code","metadata":{"id":"bONmn9YynZ3B"},"source":["# Report Plotting\n","def plotreports(*reports,metric='accuracy'):\n","  reports = pd.concat(reports).reset_index()\n","  sns.barplot(data = reports, x = 'method', hue='data', y=metric)\n","  plt.title(f'Comparing {metric}')\n","\n","# Now Plot all 3 different results\n","vs.visualize(aspect=1.5, height = 4)\n","kf.visualize(aspect=1.5, height = 4)\n","lo.visualize(aspect=1.5, height = 4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zoZdaYelj7p3"},"source":["# Plot accuracy report\n","plotreports(vs.get_report(),kf.get_report(),lo.get_report(), metric='accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LsezizCmzQCR"},"source":["# Plot loss report\n","plotreports(vs.get_report(),kf.get_report(),lo.get_report(), metric='loss')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IItU_Li1gdv6"},"source":["***EXERCISE NO. 1***\n","\n","  > **Task-4 | Score :15**"]},{"cell_type":"code","metadata":{"id":"f_2WfMugmeze"},"source":["# Now Answer following questions\n","# Answer should go below\n","\"\"\"\n","Question no. 1 : Why the Validation Set approach has a high variability?\n","\n","Question no. 2 : Why the Validation leaveoneout approach has very low variability?\n","\n","Question no. 3: Why would you prefer KFold instead of the Validation Set Approach and LeaveOneout Approach?\n","\n","Question no. 4: What is the serious limitation of the LeaveOneOut Approach?\n","\n","Question no. 5: Compare these 3 different results.\n","\n","\"\"\"\n","print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qNJ8rMbZnhBD"},"source":["***EXERCISE NO. 2***\n","\n","  > **Task-1 | Score :10**"]},{"cell_type":"code","metadata":{"id":"8EnaRMQjnlTY"},"source":["# We will create an instance of the validation set model with fit steps = 1, and fit the model\n","# We will now assign the model associated with the vs to a variable named model\n","# We use random_state = 4347 to make sure while running .run method we have a specific validation set that may not change\n","vs = ??\n","??#run with given random state\n","\n","# Now get xtrain,ytrain,xval,yval,xtest,ytest\n","# Since we used random_state=4347 in .run we need to pass this random_state=4347 argument to get xtrain,xval,ytrain,yval from train_val_split\n","xtrain, xval, ytrain, yval = ??\n","xtest,ytest = ??\n","\n","# Model you will be using from vs\n","model = ??"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NCUX18Swp_t1"},"source":["***EXERCISE NO. 2***\n","\n","  > **Task-1 | Score :10**"]},{"cell_type":"code","metadata":{"id":"IACMGSC8oCak"},"source":["# Now you have model, and you know how to access the iris dataset\n","# Now perform following task based on your previous exercise\n","\n","# 1. Calculate Two Confusion Matrices, on val and test set\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MopcFB7Tq9Jm"},"source":["# 2. Plot your confusion matrix on val set (better use seaborn plotting as shown in previous exercises)\n","#\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5oH1Hnsjq_U3"},"source":["# 3. Plot your confusion matrix on test set (better use seaborn plotting as shown in previous exercises)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mBcwtFmEoaxE"},"source":["# 4. Now Answer following questions\n","\n","\"\"\"\n","# Question no. 1: Based on your confusion matrices discuss important observations you made\n","\n","\n","\"\"\"\n","print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ojzmpF7TouDP"},"source":["***EXERCISE NO. 2***\n","\n","  > **Task-2 | Score :25**"]},{"cell_type":"code","metadata":{"id":"Mym8zXlfvGL5"},"source":["# Here we will perform One vs all approach\n","# The Iris dataset has 3 classes namely, ['setosa', 'versicolor', 'virginica']\n","# We will modify this dataset to make this 3 different datasets,\n","# First Case will be setosa vs other, i.e, setosa will be encoded as 1 and rest as 0\n","# Second Case will be setosa vs other, i.e, versicolor will be encoded as 1 and rest as 0\n","# Third Case will be setosa vs other, i.e, virginica will be encoded as 1 and rest as 0\n","# We will consider only one case where we treat this problem as setosa vs other\n","\n","x, y = load_iris(return_X_y = True)# x = preprocessing.scale(x)\n","y = 1*(y==0)\n","xtrain,xtest, ytrain, ytest = train_test_split(x,y,test_size=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_VH6oTLpvqQQ"},"source":["# Now fit a logistic regression model using sklearn\n","model = LogisticRegression()\n","\n","# Now fit this model with corresponding data from configs\n","model.fit(xtrain,ytrain)\n","\n","# Now Follow following reference\n","# https://www.daniweb.com/programming/computer-science/tutorials/520084/understanding-roc-curves-from-scratch\n","# Peform similar experiment"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-tz30HWXwmhn"},"source":["# 1. Report TPR,FPR,F1 Score\n","\n","# 2. Discuss meaning of these scores\n","\n","# 3. Plot a Predicted_probability vs Decision_boundary plot\n","\n","# 4. Plot an ROC Curve\n","\n","# 5. Discuss your important findings"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KP9FtDSV3zXm"},"source":["***EXERCISE NO. 3***\n","\n","  > **Task-1 | Score :10**"]},{"cell_type":"code","metadata":{"id":"dEpl9hc532n8"},"source":["# The following is a dataset for Online Video Characteristics and Transcoding Time Dataset Data Set from UCI machine learning datasets\n","# This dataset is much larger than the iris dataset\n","# We will use this dataset for a regression problem\n","# You can view meta data information in given link\n","import IPython\n","IPython.display.HTML(\"https://archive.ics.uci.edu/ml/datasets/Online+Video+Characteristics+and+Transcoding+Time+Dataset#\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f36Kr4HTJ5ZR"},"source":["# We have already downloaded and processed this dataset for you\n","# You can download this dataset by\n","!wget https://raw.githubusercontent.com/keshavsbhandari/CS4347/master/assignment1_regression/data/video_feature_train.csv\n","!wget https://raw.githubusercontent.com/keshavsbhandari/CS4347/master/assignment1_regression/data/video_feature_test.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mfsn3nuz6aWy"},"source":["# Now let us load this dataset\n","# You have to do nothing, this data is already processed and standardize\n","train = pd.read_csv('video_feature_train.csv')\n","test = pd.read_csv('video_feature_test.csv')\n","\n","feature_names = list(train.columns)[:-1]\n","target_names = list(train.columns)[-1:]\n","\n","\n","# Now create xtrain and xtest\n","xtrain = train.drop(columns=['label']).values\n","ytrain = train['label'].values\n","\n","xtest = test.drop(columns=['label']).values\n","ytest = test['label'].values\n","\n","# View size of data\n","print(f'xtrain : {xtrain.shape}, ytrain : {ytrain.shape}')\n","print(f'xtest : {xtest.shape}, ytest : {ytest.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cZcK0A7v6j4A"},"source":["# Now let us define a LinearRegression model\n","from sklearn.linear_model import LinearRegression\n","model = LinearRegression()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QBuOhAqx7DWD"},"source":["# Teck %%timeit check on model fitting\n","%%timeit\n","model.fit(xtrain,ytrain,)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V0Rqnk0K8cqH"},"source":["# Now let us see our model score\n","# Score represents what percentage our model explains the target variable pretty well\n","score = model.score(xtest,ytest)\n","score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-iBKxcME-qkb"},"source":["# Let us recall some basic understanding\n","# Here our model is infact following equeation\n","# model(x) = b + w0*x0 + w1*x1 + ..... + w24*x24\n","# Note: x has dim 25\n","# where b is intercept with dim 1\n","# where w0,w1 is coef_ = [w0,w1,.....,w24] of dim25"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IOFCECrS_UMD"},"source":["# Print size of coef_ and intercept\n","print(f'coef_ shape : {model.coef_.size}, intercept_shape : {model.intercept_.size}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sa8xU3bA8htX"},"source":["# Now let us see our coef_ = [w0,w1,.....]\n","model.coef_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LmTEJNTt8ou2"},"source":["# Now let us see our intercept_\n","model.intercept_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d8DGdOXC88D9"},"source":["# Let us visualize importance of these coef_\n","plt.bar(x=feature_names, height=np.abs(model.coef_), label='Feature Importance')\n","plt.xticks(rotation='vertical')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fu4qdUY789Gc"},"source":["# We can see that many features do not contribute much as they are close to 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MR8NF0tC-YBF"},"source":["# So with this understanding we come to realize that not every feature is important\n","# We have many different ways to get rid of features that are not very important\n","# One of the easiest way is itereate over all possible combinations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IxpJjIGt-irC"},"source":["# We will do subset feature selection here\n","# Subset feature selection is a way to take all possible permutations of features\n","# For our case this will be extremely bigger, so let us filter out some of weight less than mean of coef_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PYz9qfQiBWMD"},"source":["important_feature = list(np.array(feature_names)[model.coef_>np.mean(model.coef_)])\n","print(important_feature)\n","print(f'length : {len(important_feature)}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X75DY-N4DTYc"},"source":["# Let us get entire combinations\n","from itertools import  combinations\n","all_feature_combinations = []\n","for i in range(1,len(important_feature)+1):\n","  combo = [*combinations(important_feature,i)]\n","  combo = [*map(list,combo)]\n","  all_feature_combinations.extend(combo)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7PiM2ntkDdB9"},"source":["# Print length of all cominations\n","print(len(all_feature_combinations))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7EPtwI_wEID3"},"source":["# Print all feature combinations\n","all_feature_combinations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKKjiriTFAyi"},"source":["# Let us write a function that does subset selection\n","# this function takes combo, which is a list of feature_list\n","def fit_and_record(combo):\n","  record = {'subset':None,'score':-1,'model':None}\n","  record_feature_count_wise = {}\n","  for i,subset in enumerate(combo):\n","    if i%100==0:\n","      print(f'{i} of {len(combo)}')\n","    # Create a model\n","    model = ??\n","    # Select columns corr to subset and extract values from train, and test df\n","    # and assign to xtrain and xtest\n","    xtrain = ??\n","    xtest = ??\n","\n","    # Fit model with xtrain and ytrain\n","    ??\n","\n","    # calculate score\n","    score = ??\n","\n","    if len(subset) in record_feature_count_wise:\n","      record_feature_count_wise[len(subset)].append(score)\n","    else:\n","      record_feature_count_wise[len(subset)] = [score,]\n","\n","    # Write a condition where if current score is greater update subset, model and score in record\n","    if record.get('score')<score:\n","      record['subset'] = subset\n","      record['model'] = model\n","      record['score'] = score\n","  record_feature_count_wise = {i:np.array(j).mean() for i,j in record_feature_count_wise.items()}\n","  return record,record_feature_count_wise\n","\n","result,count_wise_score = fit_and_record(all_feature_combinations)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s_fIkGRUFu2D"},"source":["# Print the result\n","print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tP47cOf0Lruo"},"source":["# Print count_wise_score\n","print(count_wise_score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yVPXHa8XFxFU"},"source":["# Let us visualize importance of these coef_\n","plt.bar(x=result['subset'], height=np.abs(result['model'].coef_), label='Feature Importance')\n","plt.xticks(rotation='vertical')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5jMaFvo-M6e0"},"source":["# Let us visualize count_wise_score\n","plt.scatter(x=count_wise_score.keys(), y=count_wise_score.values(), label='Count Wise Score')\n","plt.xticks(rotation='vertical')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bjj40t3dfGMA"},"source":["***EXERCISE NO. 3***\n","\n","  > **Task-2 | Score :15**"]},{"cell_type":"code","metadata":{"id":"Ym1y_hbYIuFp"},"source":["# Now answer following\n","\n","\"\"\"\n","Q1. What important difference did you notice in two feature importance plots? How does this importance change?\n","\n","\n","Q2. What important trend do you notice in feature_count_wise_score?\n","\n","\n","Q3. Why do you think this method is not a good approach? Write two important points.\n","\n","\n","\"\"\"\n","print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jtM7BgxJVkqw"},"source":["***EXERCISE NO. 3***\n","\n","  > **Task-3 | Score :10**"]},{"cell_type":"code","metadata":{"id":"IxiL3L-XOr_d"},"source":["# IMPORTANT!!!!!!!!!!!!!! PLEASE PAY ATTENTION ONLY IN INSTRUCTIONS\n","# YOU DON'T HAVE TO CODE VERYTHING\n","# JUST TRY TO FIGURE OUT FEW THINGS AS INSTRUCTED\n","\n","\n","# Since you have now realized the above method is not a good one\n","# Recall lecture from class, where we talked about forward feature selection\n","# Now we will sequentially increase our feature space\n","# We will start with best_feature = []\n","# We will add first feature, store its score\n","# We then add next feature, and see if score is high or less\n","# If score is high we will consider it in our best_feature otherwise we will drop(pop) it\n","# We will continue this until we finished iterating over all the feature space\n","\n","# Since you have now realized above method is not a good one\n","# Recall lecture from class, where we talked about forward feature selection\n","# Now we will sequentially increase our feature space\n","# We will start with best_feature = []\n","# We will add first feature, store its score\n","# We then add next feature, and see if score is high or less\n","# If score is high we will consider it in our best_feature otherwise we will drop(pop) it\n","# We will continue this until we finished iterating over all the feature space\n","\n","# FOLLOWING FUNCTION WILL fit the given candidate features and return score and model coefficient\n","# example candidate_best_features = ['feature1', 'feature2',...] from feature_names\n","from graphviz import Digraph\n","\n","def fit_and_return_score(candidate_best_features):\n","  xtrain = train[candidate_best_features].values\n","  xtest = test[candidate_best_features].values\n","  if len(xtrain.shape) == 1:\n","    xtrain = xtrain.reshape(-1,1)\n","    xtest = xtest.reshape(-1,1)\n","  model = LinearRegression()\n","  model.fit(xtrain,ytrain)\n","  return model.score(xtest,ytest), model.coef_\n","\n","\n","# This will take existing best features, candidate features that could be possible best feature and weird network graph object\n","# You will understand why we use this network graph object later\n","# You don't need to implement everything inside the greedy function\n","# Try to fill the remaining code section as instructed\n","\n","\n","def greedy(existing_best_feature, candidate_feature_list,process):\n","  best_feature = None\n","  if existing_best_feature:\n","    score, coef_ = fit_and_return_score(existing_best_feature)\n","  else:\n","    score = -1\n","\n","  best_candidate = None\n","  for i,feature in enumerate(candidate_feature_list):\n","    score_, coef_ = fit_and_return_score(existing_best_feature + [feature])\n","\n","    # Do not pay attention here,\n","    # You are welcome to explore what I am doing here\n","    # But this is just for recording our search in terms of graph\n","    # You will play with this graph later\n","\n","    if existing_best_feature:\n","      process.edge(existing_best_feature[-1]+'_'+str(len(existing_best_feature)+1),feature+'_'+str(len(existing_best_feature)+1))\n","      process.edge(feature+'_'+str(len(existing_best_feature)+1), f'Score = {score_:.6f}')\n","    else:\n","      process.edge(feature, f'Score = {score_:.6f}')\n","\n","    # Here if current score that is newly calculated is less than old one get inside a condition block and do as instructed\n","    if score < score_:\n","      # if above condition as instructed is true\n","      # do following\n","      #1. update old score to new score\n","      #2. update best_feature to current feature\n","      ??\n","      ??\n","\n","  if best_feature:\n","    # If we were able to get our best feature we have to do something here\n","    # Do following\n","    # 1. remove best_feature from candidate_feature_list (use .remove method, see this method from list)\n","    # 2. append best_feature to existing_best_feature\n","\n","    ??\n","    ??\n","    #Please do not pay attention on following code\n","    process.edge(f'Score = {score:.6f}',best_feature+'_'+str(len(existing_best_feature)+1))\n","\n","  else:\n","    # If we are not able to get any best_feature than we have to terminate our search\n","    # Best way to do this is make our candidate_feature_list None or empty list\n","    # Do following\n","    # 1. make candidate_feature_list None\n","    ??\n","    # We need to reset our model with old existing_best_feature\n","    score_, coef_ = fit_and_return_score(existing_best_feature)\n","\n","  return existing_best_feature, candidate_feature_list, coef_, process\n","\n","\n","def forward_stepwise_selection(features):\n","  best_feature = []\n","  coef = None\n","  score = -1\n","  process = Digraph('FSW')\n","  process.node(f'Score = {0}')\n","  process.edges([(f'Score = {0}',f) for f in features])\n","\n","  # Here we will run our while loop unless we run out of features(which infact is candidate_feature_list)\n","  # Or unless our search is terminated explicitly during greedy search\n","  # So run while loop unless we have something in features\n","  # complete while ?? , ?? should be a condition that is true if there is something in features\n","  while ??:\n","    best_feature, features, coef,process = greedy(best_feature.copy(), features.copy(), process)\n","  return best_feature, coef,process\n","\n","best_feature, coef,_ = forward_stepwise_selection(feature_names.copy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vv-5JLNklLBa"},"source":["# You might be wondering how things workout in above example\n","# Now we will play with our graph object called as process\n","# You don't have to code anything here\n","# Just play with given code\n","# You can tweak number of features\n","# We have 25 feature names choose number from 1 to 25 to play with the graph\n","# If you choose smaller number graph will be smaller\n","# If you choose larger number graph will be larger\n","# You have to scroll right-left top-down to see whats going on in graph\n","# This graph is basically showing tree traversal\n","# Where we are looping over each combination of features one by one progressively from 0 to 1 to 2 to 3 to 4..... and so on\n","# You will see we will pick up only the max score generating feature sets\n","\n","# You can run this cell as many times as you like and will get different results\n","import random\n","number_of_feature_you_want_to_play_with = 15\n","best_feature_, _,process = forward_stepwise_selection(random.sample(feature_names.copy(),number_of_feature_you_want_to_play_with))\n","print(best_feature_)\n","# You might see '_someNumber' in feature names, these number represents tree level or height\n","process"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PVKKtyhIOoco"},"source":["# Now plot coef and best_feature in barplot\n","plt.bar(x = feature_names, height = np.abs(coef),label = 'Forward Step Wise')\n","plt.xticks(rotation='vertical')\n","# plt.bar(x = range(25), height = f_regression(xtrain,ytrain)[1],label = 'Important')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IS6BcimGlIX2"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-r49oaNufPAP"},"source":["***EXERCISE NO. 3***\n","\n","  > **Task-4 | Score :5**"]},{"cell_type":"code","metadata":{"id":"ZzVoR2XcO_tE"},"source":["# Answer following question\n","\n","\"\"\"\n","Q1. What is the advantage of step-wise forward feature selection vs subset feature selection method?\n","\n","\n","\n","\"\"\"\n","print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L-BCcpYnV9Ye"},"source":["***EXERCISE NO. 4***\n","\n","  > **Task-1 | Score :10**"]},{"cell_type":"code","metadata":{"id":"99SEpACVV_4-"},"source":["# So far we see some manual way of doing feature selection\n","# Now we will try to understand two very important methods for feature selection\n","# In this first assignment we will study Ridge Regression - L2 Rgularization\n","# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html\n","from sklearn.linear_model import  RidgeCV\n","\n","def ridge_regr(min_alpha = 1e-3, max_alpha = 1, count = 100):\n","  # Create a linearly spaced numpy array with given min , max and count\n","  alphas = ??\n","  # create ridge model with given alphas\n","  ridge = ??\n","  # Fit model\n","  ??\n","  print('score ',ridge.score(xtest,ytest))\n","  return ridge.coef_\n","\n","coef = ridge_regr()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DN_3NpRFXIGr"},"source":["# Now plot coef and best_feature in barplot\n","plt.bar(x = feature_names, height = np.abs(coef),label = 'Forward Step Wise')\n","plt.xticks(rotation='vertical')\n","# plt.bar(x = range(25), height = f_regression(xtrain,ytrain)[1],label = 'Important')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XdYgz10MZqkh"},"source":["***EXERCISE NO. 4***\n","\n","  > **Task-2 | Score :10**"]},{"cell_type":"code","metadata":{"id":"1tHxOW0CXXX-"},"source":["# So far we see some manual way of doing feature selection\n","# Now we will try to understand two very important methods for feature selection\n","# In this first assignment we will study Lasso Regression - L1 Rgularization\n","# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html\n","from sklearn.linear_model import  LassoCV\n","\n","def lasso_regr(min_alpha = 1e-3, max_alpha = 1, count = 100):\n","  # Create a linearly spaced numpy array with given min , max and count\n","  alphas = ??\n","  # create lasso model with given alphas\n","  lasso = ??\n","  # fit your model with xtrain and ytrain\n","  ??\n","  print('score ',lasso.score(xtest,ytest))\n","  return lasso.coef_\n","\n","coef = lasso_regr()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2_vvt6aQZKL4"},"source":["# Now plot coef and best_feature in barplot\n","plt.bar(x = feature_names, height = np.abs(coef),label = 'Forward Step Wise')\n","plt.xticks(rotation='vertical')\n","# plt.bar(x = range(25), height = f_regression(xtrain,ytrain)[1],label = 'Important')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CyvxmYoLaHal"},"source":["***EXERCISE NO. 4***\n","\n","  > **Task-3 | Score :25**"]},{"cell_type":"code","metadata":{"id":"afaeqoyMaIrt"},"source":["# Answer following question\n","\n","\"\"\"\n","Question no.1 What is the key difference between L2 (Ridge) and L1 (LASSO) approach?\n","\n","Question no.2 What is your key observation on Task1 and Task2 on Ex4?\n","\n","Question no.3 How is the bias/variance trade-off related with Task1 and Task2?\n","\n","Question no.4 What are the advantages of Task2 and Task3 methods in Ex4 compared to methods in Ex3?\n","\n","Question no.5 What are disadvantages of above two methods(L2 and L1)?\n","\n","\n","\"\"\""],"execution_count":null,"outputs":[]}]}