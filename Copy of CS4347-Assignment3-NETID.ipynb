{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of CS4347-Assignment3-NETID.ipynb","provenance":[{"file_id":"1bUVTGtdPQASsA0heGjrU-N3WmBWuB6u_","timestamp":1581629827750},{"file_id":"1rGXFqRqm039V8CFbgHhJIelkvjMtjccM","timestamp":1581473139504},{"file_id":"1LvZxCmUnjHLiPXq5jdvyQionnTMMYItf","timestamp":1581323022958}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8iwaee_T-Urj","colab_type":"text"},"source":["# ASSIGNMENT 3 - INTRO TO MACHINE LEARNING | Logistic Regression, LDA and QDA\n","\n","\n","> **FULL MARKS = 200**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IVKgB-iQ-nYY","colab_type":"text"},"source":["**Note:** To submit the assignment, please follow the same steps and in assignments 1 & 2.\n","\n","In this assignment we will use things we have learned from previous exercises. \n","\n","This assignment is comparitively more difficult than the previous modules. We will implement everything from scratch in this assignment. You will not be given instructions on how to load data, plot data, standardize/normalize data, how to write functions and many more. You will be given instructions on what you will be doing. **SO PLEASE START THIS ASSIGNMENT AS EARLY AS POSSIBLE**\n","\n","1. **Implementing Logistic Regression from Scratch | 24 TASKS - SCORE : 115**\n","    \n","    References\n","    > https://en.wikipedia.org/wiki/Logistic_regression\n","\n","    > https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc\n","\n","    > https://hackernoon.com/logistic-regression-in-python-from-scratch-954c0196d258\n","\n","    > https://medium.com/@martinpella/logistic-regression-from-scratch-in-python-124c5636b8ac\n","\n","    > https://stackabuse.com/understanding-roc-curves-with-python/\n","\n","    > https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html\n","\n","    > https://datascience.stackexchange.com/questions/40067/confusion-matrix-three-classes-python\n","\n","    > https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html\n","\n","    > https://getaravind.com/blog/confusion-matrix-seaborn-heatmap/\n","\n","    > https://docs.scipy.org/doc/numpy/reference/generated/numpy.hsplit.html\n","\n","\n","\n","2. **Understanding and implementing LDA from sratch | 3 TASKS - SCORE : 55**\n","  \n","  References\n","  > https://www.statisticssolutions.com/discriminant-analysis/\n","\n","  > https://en.wikipedia.org/wiki/Linear_discriminant_analysis\n","\n","  > https://towardsdatascience.com/linear-discriminant-analysis-in-python-76b8b17817c2\n","\n","  > https://scikit-learn.org/0.16/modules/generated/sklearn.lda.LDA.html\n","\n","3. **Understanding and implementing LDA and QDA using scikitlearn | 6 TASKS - SCORE : 30**\n","  \n","  References\n","  > https://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html#sphx-glr-auto-examples-classification-plot-lda-qda-py\n"]},{"cell_type":"markdown","metadata":{"id":"NsUM8-i_KhK1","colab_type":"text"},"source":["### 1. Implementing Logistic Regression from Scratch\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vrjNEsboAF7-","colab_type":"text"},"source":["***EXERCISE NO. 1***\n","\n","> **SCORE : 65**\n","\n"]},{"cell_type":"code","metadata":{"id":"3-mJhGJ3IWEx","colab_type":"code","colab":{}},"source":["# The following libraries are imported for you\n","import numpy as np\n","from sklearn.datasets import load_iris\n","import matplotlib.pyplot as plt\n","# I found following website really useful\n","# https://brohrer.github.io/matplotlib_framing.html\n","import pandas as pd\n","\n","# See details of this dataset @\n","# https://en.wikipedia.org/wiki/Iris_flower_data_set"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"v4586H9DK9lS"},"source":["*EXERCISE NO. 1 : TASK 1*\n","\n","> **SCORE : 0.5*7**\n","\n"]},{"cell_type":"code","metadata":{"id":"S38ZY-V9KcNL","colab_type":"code","colab":{}},"source":["# Write code below each comment to accomplish the given task\n","\n","# load the iris flower dataset\n","\n","\n","# This data is in dictionary format print all the keys\n","\n","\n","\n","# Lets print the data description\n","\n","\n","\n","# Lets print the feature names\n","\n","\n","\n","\n","# Lets print the target_names\n","\n","\n","\n","\n","# Lets print target shape (dimenssions)\n","\n","\n","\n","# Let print data(FEATURE) shape (dimenssions)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PGMpvkuALDJi"},"source":["*EXERCISE NO. 1 : TASK 2*\n","\n","> **SCORE : 0.5X3=1.5**\n","\n"]},{"cell_type":"code","metadata":{"id":"lqDQ2YK4KgWS","colab_type":"code","colab":{}},"source":["# Write code below each comment to accomplish the task\n","\n","# Import pandas and use feature_names as column, data as values to create dffeature\n","dffeature = \n","\n","# Create dftarget dataframe using target as data and ['label'] as column\n","dftarget = \n","\n","# The target labels in the original dataset are marked as 0, 1, 2. Let's replace\n","# the numbers with the actual flower species names ('setosa', 'versicolor', 'virginica').\n","# E.g. replace all 0s with the 0th value of iris.target_names, all 1s with the 1st value \n","# and so on in dftarget.\n","# Hint use the DataFrame.apply() method\n","dftarget = "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"niSy5k_GLHC9"},"source":["*EXERCISE NO. 1 : TASK 3*\n","\n","> **SCORE : 0.5X4=2**\n","\n"]},{"cell_type":"code","metadata":{"id":"ano734-lKkNz","colab_type":"code","colab":{}},"source":["# Write code below each comment to accomplish the task\n","\n","# Now concatenate column-wise and create new dataframe df contataining both\n","# the features and the target values. Use the argument sort=False\n","df = \n","\n","# Print the head of the data\n","\n","\n","# Print the tail of the data\n","\n"," \n","\n","# Check if there are any NA values\n","\n","\n","# Drop rows which contain missing values.\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dCcaCxJuM5f6","colab_type":"code","colab":{}},"source":["# Now import seaborn and plot data so that you can visualize pairplot\n","# Important note use color/hue \n","# https://seaborn.pydata.org/generated/seaborn.pairplot.html\n","# Task 1 use hue as label \n","import seaborn as sns"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_ew05V28LNri"},"source":["*EXERCISE NO. 1 : TASK 4*\n","\n","> **SCORE : 2**\n","\n"]},{"cell_type":"code","metadata":{"id":"9koP3fNLOFq6","colab_type":"code","colab":{}},"source":["# Plot the feautres in a pairplot diagram. Use params hue='label' and kind='scatter'\n","sns.pairplot(??)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"K_ha7C6HLRvl"},"source":["*EXERCISE NO. 1 : TASK 5*\n","\n","> **SCORE : 0.5**\n","\n"]},{"cell_type":"code","metadata":{"id":"LHA6OwiZOGhH","colab_type":"code","colab":{}},"source":["# Repeat the above using the value 'reg' (regression) for the parameter kind \n","sns.pairplot(??)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"09dEDgzALV3Z"},"source":["*EXERCISE NO. 1 : TASK 6*\n","\n","> **SCORE : 0.5**\n","\n"]},{"cell_type":"code","metadata":{"id":"Y2AFsKVaOTJJ","colab_type":"code","colab":{}},"source":["# Repeat the above using the value 'reg' (regression) for the parameter kind and \n","# add the parameter diag_kind='hist'\n","sns.pairplot(??)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YoWWLTP4LZdw"},"source":["*EXERCISE NO. 1 : TASK 7*\n","\n","> **SCORE : 5**\n","\n"]},{"cell_type":"code","metadata":{"id":"7hOAXemuPvOU","colab_type":"code","colab":{}},"source":["# Now you might have understood something about the dataset\n","# What are your 3 major observation??\n","# Your answer goes here\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TaiveqqbLdHB"},"source":["*EXERCISE NO. 1 : TASK 8*\n","\n","> **SCORE : 1**\n","\n"]},{"cell_type":"code","metadata":{"id":"8e5CEm_wQBkg","colab_type":"code","colab":{}},"source":["# Now use the sklearn data train test split functionality to split data into train\n","# and test parts with 80/20% split.\n","from sklearn.model_selection import train_test_split\n","xtrain, xtest, ytrain, ytest = "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9TseK7KyQNvS","colab_type":"code","colab":{}},"source":["# If you have gone through some of the details of logistic regression\n","# You might have come accross sigmoid function"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pea99igDQ8sr","colab_type":"text"},"source":["![sigmoid](https://www.researchgate.net/profile/Knut_Kvaal/publication/239269767/figure/fig2/AS:643520205430784@1530438581076/An-illustration-of-the-signal-processing-in-a-sigmoid-function.png)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WIy1fVsLLib9"},"source":["*EXERCISE NO. 1 : TASK 9*\n","\n","> **SCORE : 5**\n","\n"]},{"cell_type":"code","metadata":{"id":"_96ZQy-ZQ-jb","colab_type":"code","colab":{}},"source":["# Now using numpy we implement a sigmoid function that squishes any given input to between 0 and 1\n","# NOTE: x may be a scalar, a vector, or a matrix. The function works in all cases\n","# by using element-wise arithmetic operations (i.e. './' and '.+')\n","import numpy as np\n","\n","def sigmoid(x):\n","  return ??\n","\n","\n","# Now here use np.arange function to create an array values from -50 to 50. \n","# Use step size as 0.1 and pass this to sigmoid and store the output in the sigout variable\n","x = ??\n","\n","sigout = ??\n","\n","# print max and min of x \n","\n","\n","# print max and min of sigout\n","\n","\n","\n","# Now use matplotlib to plot x in x and sigout in y\n","\n","\n","\n","\n","# You might have understood something from above experiment\n","# Write two important things you have noticed about sigmoid function\n","# Your answer goes here\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"67Fzo4-2R5WT","colab_type":"code","colab":{}},"source":["# Now lets talk about the loss function in logistic regression"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Oi5RUYATj6a","colab_type":"text"},"source":["![alt text](https://userweb.cs.txstate.edu/~k_b459/logi.png)"]},{"cell_type":"markdown","metadata":{"id":"W5jIs_IvG_my","colab_type":"text"},"source":["In the formula above $ h_\\theta(x^{(i)}) = \\hat{y}^{(i)} $ is the predicted value for observation $ i $, whereas, $ y^{(i)} $ is the observed value in the dataset. $ h_\\theta $ is the regression function."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RuGK0Le1LnzV"},"source":["*EXERCISE NO. 1 : TASK 10*\n","\n","> **SCORE : 10**\n","\n"]},{"cell_type":"code","metadata":{"id":"naIuOqU6SB0y","colab_type":"code","colab":{}},"source":["# Implement above logistic loss function as logistic_loss. \n","# Remember your input will be the vectors y and y_hat, here h_theta_xi is y_hat and y_i is y\n","# The above loss is also known as binary cross entropy loss\n","# https://en.wikipedia.org/wiki/Cross_entropy\n","def logistic_loss(y, y_hat):\n","  return ??"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1lelUVfZLriH"},"source":["*EXERCISE NO. 1 : TASK 11*\n","\n","> **SCORE : 1**\n","\n"]},{"cell_type":"code","metadata":{"id":"PE2S5BWqT8Me","colab_type":"code","colab":{}},"source":["# Now pay attention to following important instructions\n","# Lets take a  simple example where x has size (100, 1) and y has size (100,1) here 100 is just the number of instances\n","# Important thing is number '1' which is actual dimension of x and y (coincident-here our x and y has same dimension)\n","# So now we can formulate this problem as linear regression problem y = w*x + b\n","# Now assume everything as matrix multiplication and additon\n","\n","# Now pay attention to following few examples\n","import numpy as np\n","x = np.random.randn(100,1) #my data set has 100 instances and 1 feature\n","y = np.random.randint(0,2,(100,1)) # my data set target with dimension 1\n","w = np.random.randn(1,1)\n","b = np.random.randn(1,1)\n","\n","y_hat = x@w + b #this is my linear model\n","print(y_hat.shape) # this will be (100,1)\n","print(y_hat.shape == y.shape) # this should be true\n","\n","# Lets take another example\n","x = np.random.randn(100,4) #my data set has 100 instances and 4 feature\n","y = np.random.randint(0,2,(100,1)) # my data set target w-np.mean(y*np.log(y_hat)+(1-y)*np.log(1-y_hat))ith dimension 1\n","w = np.random.randn(4,1)\n","b = np.random.randn(1,1)\n","\n","y_hat = x@w + b #this is my linear model\n","print(y_hat.shape) # this will be (100,1)\n","print(y_hat.shape == y.shape) # this should be true\n","\n","\n","# Lets take another example\n","x = np.random.randn(100,1) #my data set has 100 instances and 1 feature\n","y = np.random.randint(0,2,(100,3)) # my data set target with dimension 3\n","w = np.random.randn(1,3)\n","b = np.random.randn(1,3)\n","\n","y_hat = x@w + b #this is my linear model\n","print(y_hat.shape) # this will be (100,3)\n","print(y_hat.shape == y.shape) # this should be true\n","\n","# So now you can relate this to our problem\n","# our data has 4 features and label has 3 dimension \n","# so it is now what random w and b should be created\n","# your code goes here\n","w = np.random.randn(4,3)\n","b = np.random.randn(1,3)\n","\n","# But if you see iris.target data shape it is (?,1) \n","# If you see unique values in iris.target they are 0,1,2\n","print(iris.target.shape)\n","print(np.unique(iris.target))\n","\n","# Now considering this you have to change our real x and y \n","# There is nothing to change for x\n","X = iris.data\n","Y = np.eye(np.max(iris.target)+1)[iris.target]\n","\n","# Now use again train test split to create new xtrain,xtest, ytrain and ytest\n","xtrain, xtest, ytrain, ytest = ??"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BNX9llVhL49c"},"source":["*EXERCISE NO. 1 : TASK 12*\n","\n","> **SCORE : 20**\n","\n"]},{"cell_type":"code","metadata":{"id":"Q_cP8EXZblGk","colab_type":"code","colab":{}},"source":["# Now let us create our model \n","# use lambda function to create a model\n","# this must first compute dot product of x and w then add b and pass this result to sigmoid and return from model\n","from tqdm import tqdm\n","\n","class Model:\n","  def __init__(self, w, b, epoch=50, learning_rate = 1e-3):\n","    self.w = w\n","    self.b = b\n","    self.epoch = epoch\n","    self.learning_rate = 1e-3\n","  \n","  def sigmoid(self, x):\n","    # implement sigmoid function\n","    return ??\n","\n","  def logistic_loss(self,y, y_hat):\n","    # implement logistic loss\n","    return ??\n","\n","  def predict(self, x):\n","    # return argmax of output from probability from dim 1\n","    return ??\n","  \n","  def probability(self,x):\n","    # probability is simply dot product of x and w plus b\n","    return ??\n","\n","  def fit(self, x, y, xtest, ytest, atevery=50):\n","\n","    record = {'train':{'loss':[],'accuracy':[]},'test':{'loss':[],'accuracy':[]}}\n","    stream = tqdm(range(self.epoch))\n","\n","    for epoch in stream:\n","      # z isa probability\n","      z = \n","      # loss is a logistic loss\n","      train_loss = \n","      # dz as error which is difference of probability with real output\n","      dz = \n","      # m is length of data which is actually length of error(dz)\n","      m = \n","\n","      # partial dw or derivative of dw is actually the dot product between x and error(dz) divided by m\n","      dw = \n","      # db is sum of dz across 0 dimension, use keepdims=True while calculating sum\n","      db = \n","\n","      # Update the w it is difference between current and product of partial weight and learning rate\n","      self.w = self.w - ???\n","      # Update the b it is difference between current and product of partial weight and learning rate\n","      self.b = self.b - ???\n","\n","      # Print every 50 steps\n","      if epoch%atevery==0:\n","        record['train']['loss'].append(train_loss)\n","        record['train']['accuracy'].append(self.score(x,y))\n","        loss = self.logistic_loss(ytest, self.probability(xtest))\n","        accuracy = self.score(xtest,ytest)\n","        record['test']['loss'].append(loss)\n","        record['test']['accuracy'].append(accuracy)\n","        stream.set_postfix_str(f'loss : {loss:.2f}, accuracy : {accuracy:.2f}')\n","    return record\n","\n","  def score(self, xtest, ytest):\n","    # use np.argmax along dimension 1 to compute ylabel values that may be 0,1,or 2\n","    ylabel = \n","    # use predict function on xtest to calculate ylabel_\n","    ylabel_ = \n","    # matches is == comparision between ylabel and ylabel_\n","    matches = \n","\n","    # m is length of matches\n","    m = \n","\n","    # accuracy is 100* (sum of all matches divided by ,)\n","    accuracy = ??\n","    # return accuracy\n","    return ??"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OXS8KFsT1ZEb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q71NX9bUdD68","colab_type":"code","colab":{}},"source":["# We will define two important hyper-parameters here: epoch and learning rate\n","# https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n","\n","# Now we will instantiate our model with w,b we used above and use default epoch size and learning rate\n","model = Model(w,b)\n","# fit the model\n","record = model.fit(xtrain, ytrain, xtest, ytest,1)\n","\n","from pylab import rcParams\n","rcParams['figure.figsize'] = 18, 6\n","\n","# now plot losses the data in result with legend for train and test\n","plt.subplot(1,2,1)\n","# Requirements for the plot\n","\"\"\"\n","1.Must have legend\n","2.Must have xlabel and ylabel\n","3.Must have grid enabled\n","4.Must have sub-title\n","\"\"\"\n","\n","plt.plot(record['train']['loss'], label = 'train')\n","plt.plot(record['test']['loss'], label = 'test')\n","plt.title('loss : 50 epohc')\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.legend()\n","plt.grid()\n","plt.plot()\n","\n","# now plot accuracy the data in result with legend for train and test\n","ax = plt.subplot(1,2,2)\n","# Requirements for the plot\n","\"\"\"\n","1.Must have legend\n","2.Must have xlabel and ylabel\n","3.Must have grid enabled\n","4.Must have sub-title\n","\"\"\"\n","\n","plt.plot(record['train']['accuracy'], label = 'train')\n","plt.plot(record['test']['accuracy'], label = 'test')\n","plt.title('accuracy : 50 epoch')\n","plt.xlabel('epoch')\n","plt.ylabel('accuracy')\n","plt.legend()\n","plt.grid()\n","\n","# This will add main title\n","plt.suptitle(f'Default')\n","plt.plot()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s_RS0KVrhvpH","colab_type":"code","colab":{}},"source":["# Now repeat similar plot for following configurations\n","config_1 = {'w':np.random.randn(4,3),'b':np.random.randn(1,3),'epoch':500,  'learning_rate' : 1e-3,}\n","config_2 = {'w':np.random.randn(4,3),'b':np.random.randn(1,3),'epoch':5000, 'learning_rate' : 1}\n","config_3 = {'w':np.random.randn(4,3),'b':np.random.randn(1,3),'epoch':5000, 'learning_rate' : 1e-1}\n","config_4 = {'w':np.random.randn(4,3),'b':np.random.randn(1,3),'epoch':5000, 'learning_rate' : 1e-7}\n","config_5 = {'w':np.random.randn(4,3),'b':np.random.randn(1,3),'epoch':5000, 'learning_rate' : 3}\n","config_6 = {'w':np.random.randn(4,3),'b':np.random.randn(1,3),'epoch':5000, 'learning_rate' : 10}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0S8FhZKPMAyR"},"source":["*EXERCISE NO. 1 : TASK 13*\n","\n","> **SCORE : 5**\n","\n"]},{"cell_type":"code","metadata":{"id":"pxbfN4Er1xNG","colab_type":"code","colab":{}},"source":["# It is useless to write long code again and again so lets write a function that takes following arguments\n","# So we will write a funciton  that takes above configuration and return a plot\n","def plotwithconfig(config):\n","  model = Model(**config)\n","  record = model.fit(xtrain, ytrain, xtest, ytest)\n","  print(record.keys())\n","  # Now study above code and fill this for loop\n","  # You must fullfill plot requirements as asked above\n","  for i,l in enumerate(['loss','accuracy']):\n","    ??\n","    ??\n","\n","  plt.suptitle(f\"epoch : {config.get('epoch')}, learning_rate : {config.get('learning_rate')}\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LNaQln3bMFHh"},"source":["*EXERCISE NO. 1 : TASK 14*\n","\n","> **SCORE : 0.5x6=3**\n","\n"]},{"cell_type":"code","metadata":{"id":"W4RvZzlM6qH9","colab_type":"code","colab":{}},"source":["# Plot config1\n","plotwithconfig(config_1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W_FZJ9gc-h6r","colab_type":"code","colab":{}},"source":["# Plot config2\n","plotwithconfig(config_2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6fraPp2P-j92","colab_type":"code","colab":{}},"source":["# Plot config3\n","plotwithconfig(config_3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OiJTjjY0-me6","colab_type":"code","colab":{}},"source":["# Plot config4\n","plotwithconfig(config_4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5IFVucRH-pVz","colab_type":"code","colab":{}},"source":["# Plot config5\n","plotwithconfig(config_5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V8vMAlw3-sgp","colab_type":"code","colab":{}},"source":["# Plot config6\n","plotwithconfig(config_6)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4gM70TZeMMom"},"source":["*EXERCISE NO. 1 : TASK 15*\n","\n","> **SCORE : 5**\n","\n"]},{"cell_type":"code","metadata":{"id":"SmPoBYy5t3Tx","colab_type":"code","colab":{}},"source":["# Now compare all these configuration\n","# What are the two important things you observe from the experiment??\n","# Your answer goes here\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IdtZ6JiDMUz0"},"source":["*EXERCISE NO. 1 : TASK 16*\n","\n","> **SCORE : 5**\n","\n"]},{"cell_type":"code","metadata":{"id":"dC0TLRtYuDlK","colab_type":"code","colab":{}},"source":["# Now recall the data normalization techniques\n","# Normalize only x data and then do same experiment with config_4\n","# you have to compute std and mean along with axis 0 now\n","# write a function that normalize given xdata\n","# while calculating std add 1e-10 also\n","def normalize(xdata):\n","  std = xdata.std(axis=0) + 1e-10\n","  mean = xdata.mean(axis=0)\n","  return (xdata-mean)/std\n","\n","# Now you will use above function to create xtrain and xtest these are new version of xtrain and xtest\n","# No need to normalize ytrain and ytest\n","xtrain = normalize(xtrain)\n","xtest = normalize(xtest)\n","\n","\n","# Now use config_4 and do same experiment\n","plotwithconfig(config_4)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"c0WXIZzdMcKu"},"source":["*EXERCISE NO. 1 : TASK 17*\n","\n","> **SCORE : 5**\n","\n"]},{"cell_type":"code","metadata":{"id":"yr9W89Wb_GOK","colab_type":"code","colab":{}},"source":["# What difference did you get with normalization?? Was it Useful??\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"--tMNm1MIIxV"},"source":["*EXERCISE NO. 1 : TASK 18*\n","\n","> **SCORE : 3**\n","\n"]},{"cell_type":"code","metadata":{"id":"QOE8iYVXIJ_z","colab_type":"code","colab":{}},"source":["# Now from above experiment, chose appropriate hyperparameter and instantiate a model \n","# Fit your model on new xtrain, xtest i.e, normalized\n","best_epoch = ?\n","best_learning_rate = ?\n","model = Model(??)\n","result = model.fit(??)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rl7QN_9YJNs5"},"source":["*EXERCISE NO. 1 : TASK 19*\n","\n","> **SCORE : 2**\n","\n"]},{"cell_type":"code","metadata":{"id":"3bsp1iw6JOpe","colab_type":"code","colab":{}},"source":["# Now predict xtest and use confusion matrix example given in references\n","# Find confusion matrix for multi class problem \n","# Plot confusion matrix\n","# Note output of .predict will be simply label, but ytest is a one hot encoding\n","# You can use np.argmax to convert ytest from onehot to simple list of 0,1,2 where each represent different \n","# class ['setosa','versicolor','virginica'] index wise\n","ytest_pred = ?? #looks like [0 0 1 1 1 2 2 2 1 2 1 2 1 2 2 0 ..\n","# ytest looks like [[0,0,1],[1,0,0],.....] which not what we want so we need argmax indices corresponding to respective classes\n","\n","ytest_true = ?? #use np.argmax here output looks like [0 0 1 1 1 2 2 2 1 2 1 2 1 2 2 0 .. \n","\n","print(ytest_pred, ytest_true)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UPOEBs-kTjZB"},"source":["*EXERCISE NO. 1 : TASK 20*\n","\n","> **SCORE : 10**\n","\n"]},{"cell_type":"code","metadata":{"id":"1jv4tDVhTgyC","colab_type":"code","colab":{}},"source":["# Import sklearn confusion metrix\n","# Please see documentation of confusion_matrix properly and pass right labels name\n","from sklearn.metrics import confusion_matrix,plot_confusion_matrix\n","# Please Map your output to actual label name\n","labels = ['setosa','versicolor','virginica']\n","\n","ytest_pred = list(map(???))#instead of [0,1,0,2,2...] output should be ['setosa','versicolor',.....]\n","ytest_true = list(map(???))#instead of [0,1,0,2,2...] output should be ['setosa','versicolor',.....]\n","cmatrix = confusion_matrix(???) #use labels name as well"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZzefckHTTsKK"},"source":["*EXERCISE NO. 1 : TASK 21*\n","\n","> **SCORE : 10**\n","\n"]},{"cell_type":"code","metadata":{"id":"JbJFLQPcTqpI","colab_type":"code","colab":{}},"source":["# Import seaborn and use heatmap\n","# Use annot and cbar as True, and xticklabels and yticklabels should be labels we talked before\n","import seaborn as sns\n","sns.heatmap(????)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"98OQZqLFNqvv"},"source":["*EXERCISE NO. 1 : TASK 22*\n","\n","> **SCORE : 5**\n","\n"]},{"cell_type":"code","metadata":{"id":"_4mvrarnNvOW","colab_type":"code","colab":{}},"source":["# In this section we will do some area under curve or roc curve thing\n","# You can study about ROC curve from given references or more from internet\n","# To make things simpler we are going to have 3 different roc curve for 3 different classes \n","# For ROC curve we need actual probability so we will now use .probability function from model to get the probability\n","# We have to split this probability into 3 different vectors column wise\n","# We have to split ytest as well into 3 different vectors column wise you can use np.hsplit to split into 3 different columns\n","# See documentation for further info\n","\n","ytest_prob = model.???\n","prob_class1, prob_class2, prob_class3 = np.hsplit(??)\n","class1, class2, class3 = np.hsplit(??)\n","\n","# Now use following two function\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"73U0w0jgUgms"},"source":["*EXERCISE NO. 1 : TASK 23*\n","\n","> **SCORE : 5**\n","\n"]},{"cell_type":"code","metadata":{"id":"MKtHMJlZUXC_","colab_type":"code","colab":{}},"source":["# Now we will write a function to plot 3 different roc curve\n","# this function will take ytes_prob not splitted ones, ytest not splitted ones and labels as input\n","# Complete this function\n","def plot_them_all(ytest_prob, ytest, labels):\n","  probs = #split probability probs is the list of probability for class1,class2 and class3\n","  ytrues = #split ytrue \n","\n","  for i,(prob,ytrue,label) in enumerate(zip(probs,ytrues, labels)):\n","    plt.subplot(1,3,i+1)\n","\n","    fpr, tpr, thresholds = ???#use roc_curve here, see docx for info\n","    auc_score = ?? # calculate roc_curve score here\n","    \n","    print(f'auc_score for class {label} = {auc_score}')\n","    plt.plot(fpr, tpr, color='orange', label='ROC')\n","    \n","    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n","    \n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    \n","    plt.title(f'{label}')\n","    plt.legend()\n","  \n","  plt.suptitle('Receiver Operating Characteristic (ROC) Curve')\n","\n","\n","# Now finally pltothemall\n","plot_them_all(ytest_prob, ytest, labels)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZACZXFjsUptX"},"source":["*EXERCISE NO. 1 : TASK 24*\n","\n","> **SCORE : 15**\n","\n"]},{"cell_type":"code","metadata":{"id":"FMl6xr2FUlXH","colab_type":"code","colab":{}},"source":["# What are your 3 major observation in this experiment??\n","# Your answer goes here\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tgWBeH5_ElaU"},"source":["### 2. Implementing LDA from Scratch\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"h6RzfC6jElaX"},"source":["***EXERCISE NO. 2***\n","\n","> **SCORE : 15 + 20 * 2 = 55**\n","\n"]},{"cell_type":"code","metadata":{"id":"VKZg4VeyEY6A","colab_type":"code","colab":{}},"source":["# Please follow following posts\n","# https://towardsdatascience.com/linear-discriminant-analysis-in-python-76b8b17817c2\n","# This post has step wise step method to implement LDA from scratch\n","# You will be doing same task on LDA but this time you will be using iris data set\n","# Remember we have already loaded iris dataset as dataframe\n","\n","# You have to achieve following tasks\n","\n","# SCORE 15 + 20X2\n","# Task 1 : Compute class_feature_means for all the variable in our dataset\n","\n","# Task 2 : Compute eigen_values and eigen_vectors and also show Explained Variance, w_matrix and X_lda and use label encoder to fit_transform as shown in example\n","\n","# Task 3 : Finally Visualize your result\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KAUxjDJLGB4u","colab_type":"code","colab":{}},"source":["\"\"\"\n","NOTICE:\n","YOU CAN USE AS MANY CELLS AS YOU WANT FOR ABOVE QUESTION\n","\"\"\"\n","print()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"91_w40bQJfv1"},"source":["*EXERCISE NO. 2 : TASK 1*\n","\n","> **SCORE : 20**\n","\n"]},{"cell_type":"code","metadata":{"id":"rX4WG2TaJ-zg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tWlOfSc2KAAT"},"source":["*EXERCISE NO. 2 : TASK 2*\n","\n","> **SCORE : 20**\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"H247JpAeKAAW","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NRIrdva6KAcq"},"source":["*EXERCISE NO. 2 : TASK 3*\n","\n","> **SCORE : 20**\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"v1hnHyicKAcr","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Wk6xHaXYHJA5"},"source":["### 3. Understanding and implementing LDA and QDA using scikitlearn\n","---\n","\n","\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zpx2kXktHJA8"},"source":["***EXERCISE NO. 3***\n","\n","> **SCORE : 5x6=30**\n","\n"]},{"cell_type":"code","metadata":{"id":"IZnyQczTHhXP","colab_type":"code","colab":{}},"source":["# Visit given references\n","# In scikit learn you can see multiple examples of LDA and QDA \n","# Implement these examples in two versions of our data and compare the results\n","\n","\n","# Version 1, non-normalized data\n","# Version 2, normalized data\n","\n","\n","# You have to perform following task\n","\n","# SCORE : 5X2\n","# Task 1: Create non-normalized dataset from iris_data [you can use code from above], xtrain,xtest,ytrain,ytest\n","# Task 2: Create normalized dataset from iris_data [you can use code from above], nxtrain,nxtest,ytrain,ytest\n","\n","# SCORE : 10X2\n","# Task 3: Use LDA from sklearn, initialize a model and fit LDA\n","# Task 4: Use QDA from sklearn, initialize a model and fit QDA\n","\n","# SCORE : 5 + 5\n","# Task 5: Get Accuracy on both train and test set and compare this with normalized and non-normalized version\n","# Task 6: Discuss an important thing you observed in this experiment?? What is the fundamental difference between QDA and LDA?\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qhj-ns0tIeho","colab_type":"code","colab":{}},"source":["\"\"\"\n","NOTICE:\n","YOU CAN USE AS MANY CELLS AS YOU WANT FOR ABOVE QUESTION\n","\"\"\"\n","print()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ELKbJ6hKJFG1","colab_type":"text"},"source":["*EXERCISE NO. 3 : TASK 1*\n","\n","> **SCORE : 5**\n","\n"]},{"cell_type":"code","metadata":{"id":"zAMNK2OBJOqM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RYtwXu8yJmFU"},"source":["*EXERCISE NO. 3 : TASK 2*\n","\n","> **SCORE : 5**\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oB3179ytJmFW","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"scUs4SvFJUlK"},"source":["*EXERCISE NO. 3 : TASK 3*\n","\n","> **SCORE : 5**\n","\n"]},{"cell_type":"code","metadata":{"id":"WVaDbLYgJYFo","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jWZFp4jrJwQU"},"source":["*EXERCISE NO. 3 : TASK 4*\n","\n","> **SCORE : 5**\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"h6wKXf-AJwQV","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s19Vl8hDJZR_"},"source":["*EXERCISE NO. 3 : TASK 5*\n","\n","> **SCORE : 5**\n","\n"]},{"cell_type":"code","metadata":{"id":"DNl_uxSxJdeH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nW6fcNF_J2mu"},"source":["*EXERCISE NO. 3 : TASK 6*\n","\n","> **SCORE : 5**\n","\n"]},{"cell_type":"code","metadata":{"id":"f36Kr4HTJ5ZR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}